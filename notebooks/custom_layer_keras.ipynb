{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b1aa66",
   "metadata": {},
   "source": [
    "# Custom layer with `Keras`\n",
    "\n",
    "In this notebook, we implement custom layer in `keras` that performs layer normalization.\n",
    "\n",
    "`keras` has a `keras.layers.LayerNormalization` class that we will use to check our result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae094aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7372ba",
   "metadata": {},
   "source": [
    "## Custom normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37585ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNormLayer(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        shape = input_shape[:-1] + (1,) #keeps the input shape, but the last axis (features) is collapsed to a singleton\n",
    "        self.alpha = self.add_weight(shape=shape, dtype=tf.float32, initializer=keras.initializers.Constant(value=1.0))\n",
    "        self.beta  = self.add_weight(shape=shape, dtype=tf.float32, initializer=keras.initializers.Constant(value=0.0))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mu = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n",
    "        std = tf.sqrt(tf.reduce_mean(tf.square(inputs - mu), axis=-1, keepdims=True))\n",
    "        epsilon = 1e-3\n",
    "        return self.alpha * (inputs - mu) / (std + epsilon) + self.beta\n",
    "    \n",
    "    def compute_output_shape(self, input_shape): # input and output of this layer have the same shape\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bee4eb",
   "metadata": {},
   "source": [
    "## Test with a simple input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f0b31d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[ 0.5766844 ,  0.5766844 ,  0.5766844 , -1.7300532 ],\n",
       "        [ 1.2641115 ,  0.63205576, -0.63205576, -1.2641115 ],\n",
       "        [-1.5057408 ,  0.9034444 ,  0.9034444 , -0.30114815]],\n",
       "\n",
       "       [[-0.7032438 ,  1.7078779 , -0.3013902 , -0.7032438 ],\n",
       "        [ 0.9996668 ,  0.9996668 , -0.9996668 , -0.9996668 ],\n",
       "        [ 1.6027107 , -0.5342369 ,  0.        , -1.0684738 ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.constant([[[5, 5, 5, 3], [6, 5, 3, 2], [1, 3, 3, 2]],[[1, 7, 2, 1], [8, 8, 2, 2], [7, 3, 4, 2]]], dtype=tf.float32)\n",
    "my_layer = MyNormLayer()\n",
    "my_layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d85f5871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[ 0.5769658 ,  0.5769658 ,  0.5769658 , -1.7308974 ],\n",
       "        [ 1.2646582 ,  0.632329  , -0.6323291 , -1.2646582 ],\n",
       "        [-1.5064616 ,  0.9038768 ,  0.9038768 , -0.30129242]],\n",
       "\n",
       "       [[-0.70346963,  1.7084262 , -0.30148697, -0.70346963],\n",
       "        [ 0.99994445,  0.99994445, -0.9999444 , -0.9999444 ],\n",
       "        [ 1.6033385 , -0.53444624,  0.        , -1.0688924 ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_layer = keras.layers.LayerNormalization(epsilon=1e-3)\n",
    "keras_layer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45033d61",
   "metadata": {},
   "source": [
    "As the above example shows, my custom normalization layer produces an output that is close to the keras implementation (they are identical up to the third decimal place)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd4eca8",
   "metadata": {},
   "source": [
    "## Some code for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a3a9118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@keras_export(\"keras.layers.LayerNormalization\")\n",
      "class LayerNormalization(Layer):\n",
      "    \"\"\"Layer normalization layer (Ba et al., 2016).\n",
      "\n",
      "    Normalize the activations of the previous layer for each given example in a\n",
      "    batch independently, rather than across a batch like Batch Normalization.\n",
      "    i.e. applies a transformation that maintains the mean activation within each\n",
      "    example close to 0 and the activation standard deviation close to 1.\n",
      "\n",
      "    If `scale` or `center` are enabled, the layer will scale the normalized\n",
      "    outputs by broadcasting them with a trainable variable `gamma`, and center\n",
      "    the outputs by broadcasting with a trainable variable `beta`. `gamma` will\n",
      "    default to a ones tensor and `beta` will default to a zeros tensor, so that\n",
      "    centering and scaling are no-ops before training has begun.\n",
      "\n",
      "    So, with scaling and centering enabled the normalization equations\n",
      "    are as follows:\n",
      "\n",
      "    Let the intermediate activations for a mini-batch to be the `inputs`.\n",
      "\n",
      "    For each sample `x_i` in `inputs` with `k` features, we compute the mean and\n",
      "    variance of the sample:\n",
      "\n",
      "    ```python\n",
      "    mean_i = sum(x_i[j] for j in range(k)) / k\n",
      "    var_i = sum((x_i[j] - mean_i) ** 2 for j in range(k)) / k\n",
      "    ```\n",
      "\n",
      "    and then compute a normalized `x_i_normalized`, including a small factor\n",
      "    `epsilon` for numerical stability.\n",
      "\n",
      "    ```python\n",
      "    x_i_normalized = (x_i - mean_i) / sqrt(var_i + epsilon)\n",
      "    ```\n",
      "\n",
      "    And finally `x_i_normalized ` is linearly transformed by `gamma` and `beta`,\n",
      "    which are learned parameters:\n",
      "\n",
      "    ```python\n",
      "    output_i = x_i_normalized * gamma + beta\n",
      "    ```\n",
      "\n",
      "    `gamma` and `beta` will span the axes of `inputs` specified in `axis`, and\n",
      "    this part of the inputs' shape must be fully defined.\n",
      "\n",
      "    For example:\n",
      "\n",
      "    >>> layer = keras.layers.LayerNormalization(axis=[1, 2, 3])\n",
      "    >>> layer.build([5, 20, 30, 40])\n",
      "    >>> print(layer.beta.shape)\n",
      "    (20, 30, 40)\n",
      "    >>> print(layer.gamma.shape)\n",
      "    (20, 30, 40)\n",
      "\n",
      "    Note that other implementations of layer normalization may choose to define\n",
      "    `gamma` and `beta` over a separate set of axes from the axes being\n",
      "    normalized across. For example, Group Normalization\n",
      "    ([Wu et al. 2018](https://arxiv.org/abs/1803.08494)) with group size of 1\n",
      "    corresponds to a Layer Normalization that normalizes across height, width,\n",
      "    and channel and has `gamma` and `beta` span only the channel dimension.\n",
      "    So, this Layer Normalization implementation will not match a Group\n",
      "    Normalization layer with group size set to 1.\n",
      "\n",
      "    Args:\n",
      "        axis: Integer or List/Tuple. The axis or axes to normalize across.\n",
      "            Typically, this is the features axis/axes. The left-out axes are\n",
      "            typically the batch axis/axes. `-1` is the last dimension in the\n",
      "            input. Defaults to `-1`.\n",
      "        epsilon: Small float added to variance to avoid dividing by zero.\n",
      "            Defaults to 1e-3.\n",
      "        center: If True, add offset of `beta` to normalized tensor. If False,\n",
      "            `beta` is ignored. Defaults to `True`.\n",
      "        scale: If True, multiply by `gamma`. If False, `gamma` is not used.\n",
      "            When the next layer is linear (also e.g. `nn.relu`), this can be\n",
      "            disabled since the scaling will be done by the next layer.\n",
      "            Defaults to `True`.\n",
      "        rms_scaling: If True, `center` and `scale` are ignored, and the\n",
      "            inputs are scaled by `gamma` and the inverse square root\n",
      "            of the square of all inputs. This is an approximate and faster\n",
      "            approach that avoids ever computing the mean of the input. Note that\n",
      "            this *isn't* equivalent to the computation that the\n",
      "            `keras.layers.RMSNormalization` layer performs.\n",
      "        beta_initializer: Initializer for the beta weight. Defaults to zeros.\n",
      "        gamma_initializer: Initializer for the gamma weight. Defaults to ones.\n",
      "        beta_regularizer: Optional regularizer for the beta weight.\n",
      "            None by default.\n",
      "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
      "            None by default.\n",
      "        beta_constraint: Optional constraint for the beta weight.\n",
      "            None by default.\n",
      "        gamma_constraint: Optional constraint for the gamma weight.\n",
      "            None by default.\n",
      "        **kwargs: Base layer keyword arguments (e.g. `name` and `dtype`).\n",
      "\n",
      "\n",
      "    Reference:\n",
      "\n",
      "    - [Lei Ba et al., 2016](https://arxiv.org/abs/1607.06450).\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        axis=-1,\n",
      "        epsilon=1e-3,\n",
      "        center=True,\n",
      "        scale=True,\n",
      "        rms_scaling=False,\n",
      "        beta_initializer=\"zeros\",\n",
      "        gamma_initializer=\"ones\",\n",
      "        beta_regularizer=None,\n",
      "        gamma_regularizer=None,\n",
      "        beta_constraint=None,\n",
      "        gamma_constraint=None,\n",
      "        **kwargs,\n",
      "    ):\n",
      "        super().__init__(**kwargs)\n",
      "        if isinstance(axis, (list, tuple)):\n",
      "            self.axis = list(axis)\n",
      "        elif isinstance(axis, int):\n",
      "            self.axis = axis\n",
      "        else:\n",
      "            raise TypeError(\n",
      "                \"Expected an int or a list/tuple of ints for the \"\n",
      "                \"argument 'axis', but received: %r\" % axis\n",
      "            )\n",
      "\n",
      "        self.epsilon = epsilon\n",
      "        self.center = center\n",
      "        self.scale = scale\n",
      "        self.rms_scaling = rms_scaling\n",
      "        self.beta_initializer = initializers.get(beta_initializer)\n",
      "        self.gamma_initializer = initializers.get(gamma_initializer)\n",
      "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
      "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
      "        self.beta_constraint = constraints.get(beta_constraint)\n",
      "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
      "\n",
      "        self.supports_masking = True\n",
      "        self.autocast = False\n",
      "\n",
      "    def build(self, input_shape):\n",
      "        if isinstance(self.axis, list):\n",
      "            shape = tuple([input_shape[dim] for dim in self.axis])\n",
      "        else:\n",
      "            shape = (input_shape[self.axis],)\n",
      "            self.axis = [self.axis]\n",
      "        if self.scale or self.rms_scaling:\n",
      "            self.gamma = self.add_weight(\n",
      "                name=\"gamma\",\n",
      "                shape=shape,\n",
      "                initializer=self.gamma_initializer,\n",
      "                regularizer=self.gamma_regularizer,\n",
      "                constraint=self.gamma_constraint,\n",
      "                trainable=True,\n",
      "                autocast=False,\n",
      "            )\n",
      "        else:\n",
      "            self.gamma = None\n",
      "\n",
      "        if self.center and not self.rms_scaling:\n",
      "            self.beta = self.add_weight(\n",
      "                name=\"beta\",\n",
      "                shape=shape,\n",
      "                initializer=self.beta_initializer,\n",
      "                regularizer=self.beta_regularizer,\n",
      "                constraint=self.beta_constraint,\n",
      "                trainable=True,\n",
      "                autocast=False,\n",
      "            )\n",
      "        else:\n",
      "            self.beta = None\n",
      "\n",
      "        self.built = True\n",
      "\n",
      "    def call(self, inputs):\n",
      "        # Compute the axes along which to reduce the mean / variance\n",
      "        input_shape = inputs.shape\n",
      "        ndims = len(input_shape)\n",
      "\n",
      "        # Broadcasting only necessary for norm when the axis is not just\n",
      "        # the last dimension\n",
      "        broadcast_shape = [1] * ndims\n",
      "        for dim in self.axis:\n",
      "            broadcast_shape[dim] = input_shape[dim]\n",
      "\n",
      "        def _broadcast(v):\n",
      "            if (\n",
      "                v is not None\n",
      "                and len(v.shape) != ndims\n",
      "                and self.axis != [ndims - 1]\n",
      "            ):\n",
      "                return ops.reshape(v, broadcast_shape)\n",
      "            return v\n",
      "\n",
      "        compute_dtype = backend.result_type(inputs.dtype, \"float32\")\n",
      "        # LN is prone to overflow with float16/bfloat16 inputs, so we upcast to\n",
      "        # float32 for the subsequent computations.\n",
      "        inputs = ops.cast(inputs, compute_dtype)\n",
      "\n",
      "        if self.rms_scaling:\n",
      "            # Calculate outputs with only variance and gamma if rms scaling\n",
      "            # is enabled\n",
      "            # Calculate the variance along self.axis (layer activations).\n",
      "            variance = ops.var(inputs, axis=self.axis, keepdims=True)\n",
      "            inv = ops.rsqrt(variance + self.epsilon)\n",
      "\n",
      "            outputs = (\n",
      "                inputs * inv * ops.cast(_broadcast(self.gamma), inputs.dtype)\n",
      "            )\n",
      "        else:\n",
      "            # Calculate the mean & variance along self.axis (layer activations).\n",
      "            mean, variance = ops.moments(inputs, axes=self.axis, keepdims=True)\n",
      "            gamma, beta = _broadcast(self.gamma), _broadcast(self.beta)\n",
      "\n",
      "            inv = ops.rsqrt(variance + self.epsilon)\n",
      "            if gamma is not None:\n",
      "                gamma = ops.cast(gamma, inputs.dtype)\n",
      "                inv = inv * gamma\n",
      "\n",
      "            res = -mean * inv\n",
      "            if beta is not None:\n",
      "                beta = ops.cast(beta, inputs.dtype)\n",
      "                res = res + beta\n",
      "\n",
      "            outputs = inputs * inv + res\n",
      "        return ops.cast(outputs, self.compute_dtype)\n",
      "\n",
      "    def compute_output_shape(self, input_shape):\n",
      "        if isinstance(self.axis, int):\n",
      "            axes = [self.axis]\n",
      "        else:\n",
      "            axes = self.axis\n",
      "\n",
      "        for axis in axes:\n",
      "            if axis >= len(input_shape) or axis < -len(input_shape):\n",
      "                raise ValueError(\n",
      "                    f\"Axis {axis} is out of bounds for \"\n",
      "                    f\"input shape {input_shape}. \"\n",
      "                    f\"Received: axis={self.axis}\"\n",
      "                )\n",
      "        return input_shape\n",
      "\n",
      "    def get_config(self):\n",
      "        config = {\n",
      "            \"axis\": self.axis,\n",
      "            \"epsilon\": self.epsilon,\n",
      "            \"center\": self.center,\n",
      "            \"scale\": self.scale,\n",
      "            \"rms_scaling\": self.rms_scaling,\n",
      "            \"beta_initializer\": initializers.serialize(self.beta_initializer),\n",
      "            \"gamma_initializer\": initializers.serialize(self.gamma_initializer),\n",
      "            \"beta_regularizer\": regularizers.serialize(self.beta_regularizer),\n",
      "            \"gamma_regularizer\": regularizers.serialize(self.gamma_regularizer),\n",
      "            \"beta_constraint\": constraints.serialize(self.beta_constraint),\n",
      "            \"gamma_constraint\": constraints.serialize(self.gamma_constraint),\n",
      "        }\n",
      "        base_config = super().get_config()\n",
      "        return {**base_config, **config}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(keras.layers.LayerNormalization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7aaa20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.constant([[[5, 5, 5, 3], [6, 5, 3, 2], [1, 3, 3, 2]],[[1, 7, 2, 1], [8, 8, 2, 2], [7, 3, 4, 2]]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86d41a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = tf.reduce_mean(inputs, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "723e32f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[ 0.5 ,  0.5 ,  0.5 , -1.5 ],\n",
       "        [ 2.  ,  1.  , -1.  , -2.  ],\n",
       "        [-1.25,  0.75,  0.75, -0.25]],\n",
       "\n",
       "       [[-1.75,  4.25, -0.75, -1.75],\n",
       "        [ 3.  ,  3.  , -3.  , -3.  ],\n",
       "        [ 3.  , -1.  ,  0.  , -2.  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs - mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38f314a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = tf.reduce_mean(tf.square(inputs - mu), axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b009d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 1), dtype=float32, numpy=\n",
       "array([[[0.75  ],\n",
       "        [2.5   ],\n",
       "        [0.6875]],\n",
       "\n",
       "       [[6.1875],\n",
       "        [9.    ],\n",
       "        [3.5   ]]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4ac15df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var([6, 5, 3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49602b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = keras_layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d091181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.5769658 ,  0.5769658 ,  0.5769658 , -1.7308974 ],\n",
       "        [ 1.2646582 ,  0.632329  , -0.6323291 , -1.2646582 ],\n",
       "        [-1.5064616 ,  0.9038768 ,  0.9038768 , -0.30129242]],\n",
       "\n",
       "       [[-0.70346963,  1.7084262 , -0.30148697, -0.70346963],\n",
       "        [ 0.99994445,  0.99994445, -0.9999444 , -0.9999444 ],\n",
       "        [ 1.6033385 , -0.53444624,  0.        , -1.0688924 ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb71d546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2, 3]), 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = inputs.shape\n",
    "(input_shape[:-1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35c5d5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape[:-1] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97ce8f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tup = (1, 4, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
