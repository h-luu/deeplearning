{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06e1b79b",
   "metadata": {},
   "source": [
    "# Model accuracy metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5a239c",
   "metadata": {},
   "source": [
    "In this notebook, we train a deep neural network with `keras`, and examine the training and validation accuracies reported by:\n",
    "- `fit().history`\n",
    "- `model.evaluate`\n",
    "\n",
    "It is known that the training accuracy metric produced by these two methods differ slightly.  The same problem applies to loss metric.\n",
    "\n",
    "A quick internet search brings up a few suspected reasons: difference between training/test mode in keras, learning phase, batch normalization, dropout etc. However, I haven't been able to find a definitive answer.\n",
    "\n",
    "It seems that the validation metrics always agree.  My main takeaways are then:\n",
    "\n",
    "- Always have a validation set (validation fit and accuracy seem reliable)\n",
    "- If it's necessary to calculate training fit, use `model.evaluate` rather than relying on the `fit().history` logs.\n",
    "\n",
    "For this investigation, we will use the CIFAR10 image dataset.\n",
    "\n",
    "Some references on this issue:\n",
    "\n",
    "https://github.com/keras-team/keras/issues/6977\n",
    "\n",
    "https://stackoverflow.com/questions/51123198/strange-behaviour-of-the-loss-function-in-keras-model-with-pretrained-convoluti/51124511#51124511\n",
    "\n",
    "https://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef29a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 11:11:29.097812: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-06 11:11:29.105536: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746544289.114080   24878 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746544289.116593   24878 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746544289.123658   24878 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746544289.123671   24878 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746544289.123673   24878 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746544289.123674   24878 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-06 11:11:29.125944: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c2e0a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Num GPUs Available: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"****Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f889a8",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "329de4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8906cdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_valid = X_valid.astype(\"float32\") / 255.0\n",
    "X_test = X_test.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "153e4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79f9bfb",
   "metadata": {},
   "source": [
    "## Build models\n",
    "Let's define a few functions to help us build models easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e44883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(batch_norm: bool):\n",
    "    if batch_norm:\n",
    "        layer_list = [keras.layers.Flatten(input_shape=(32,32,3))] + \\\n",
    "            [x for _ in range(20) for x in [keras.layers.Dense(100, activation=\"elu\", kernel_initializer=keras.initializers.HeNormal()), keras.layers.BatchNormalization()]] + \\\n",
    "            [keras.layers.Dense(10, activation=\"softmax\")]\n",
    "    else:\n",
    "        layer_list = [keras.layers.Flatten(input_shape=(32,32,3))] + \\\n",
    "            [keras.layers.Dense(100, activation=\"elu\", kernel_initializer=keras.initializers.HeNormal()) for _ in range(20)] + \\\n",
    "            [keras.layers.Dense(10, activation=\"softmax\")]\n",
    "\n",
    "    model = keras.models.Sequential(layer_list)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a840600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, batch_size: int, total_epochs: int):\n",
    "   opt = keras.optimizers.Nadam(learning_rate=1e-5)\n",
    "   \n",
    "   model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "         optimizer=opt,\n",
    "         metrics=[\"accuracy\"])\n",
    "   \n",
    "   history = model.fit(X_train, y_train, epochs=total_epochs,\n",
    "      batch_size=batch_size,\n",
    "      validation_data=(X_valid,y_valid))\n",
    "   return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d55f4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, batch_size):\n",
    "    _, train_accuracy = model.evaluate(X_train, y_train, batch_size=batch_size, verbose=0)\n",
    "    _, valid_accuracy = model.evaluate(X_valid, y_valid, batch_size=batch_size, verbose=0)\n",
    "    return train_accuracy, valid_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce443f7",
   "metadata": {},
   "source": [
    "## Train and evaluate\n",
    "\n",
    "We test a few models with different batch size and traiing epochs, with or without batch normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afa67628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/hieuluu/MAIN_DATA/deeplearning/venv-deeplearning/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "I0000 00:00:1746544292.759007   24878 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9177 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746544295.809432   24995 service.cc:152] XLA service 0x7e7204029e00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1746544295.809443   24995 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4070, Compute Capability 8.9\n",
      "2025-05-06 11:11:35.920384: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1746544296.102401   24995 cuda_dnn.cc:529] Loaded cuDNN version 90800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  56/1407\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0917 - loss: 4.5651"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746544297.022001   24995 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.1916 - loss: 2.4718 - val_accuracy: 0.3134 - val_loss: 1.8922\n",
      "Epoch 2/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.3240 - loss: 1.8750 - val_accuracy: 0.3468 - val_loss: 1.7893\n",
      "Epoch 3/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.3604 - loss: 1.7814 - val_accuracy: 0.3728 - val_loss: 1.7334\n",
      "Epoch 4/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.3842 - loss: 1.7218 - val_accuracy: 0.3910 - val_loss: 1.6952\n",
      "Epoch 5/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.3987 - loss: 1.6772 - val_accuracy: 0.4016 - val_loss: 1.6661\n",
      "Epoch 6/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4132 - loss: 1.6410 - val_accuracy: 0.4086 - val_loss: 1.6436\n",
      "Epoch 7/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.4240 - loss: 1.6115 - val_accuracy: 0.4196 - val_loss: 1.6264\n",
      "Epoch 8/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4330 - loss: 1.5866 - val_accuracy: 0.4242 - val_loss: 1.6118\n",
      "Epoch 9/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4410 - loss: 1.5652 - val_accuracy: 0.4292 - val_loss: 1.6001\n",
      "Epoch 10/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.4475 - loss: 1.5462 - val_accuracy: 0.4330 - val_loss: 1.5904\n",
      "Epoch 1/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.1191 - loss: 2.8282 - val_accuracy: 0.1786 - val_loss: 2.3962\n",
      "Epoch 2/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.1871 - loss: 2.3406 - val_accuracy: 0.2366 - val_loss: 2.1860\n",
      "Epoch 3/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.2386 - loss: 2.1521 - val_accuracy: 0.2660 - val_loss: 2.0915\n",
      "Epoch 4/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.2730 - loss: 2.0401 - val_accuracy: 0.2786 - val_loss: 2.0344\n",
      "Epoch 5/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.2968 - loss: 1.9652 - val_accuracy: 0.2926 - val_loss: 1.9954\n",
      "Epoch 6/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.3179 - loss: 1.9035 - val_accuracy: 0.3010 - val_loss: 1.9671\n",
      "Epoch 7/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.3366 - loss: 1.8547 - val_accuracy: 0.3052 - val_loss: 1.9494\n",
      "Epoch 8/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.3524 - loss: 1.8131 - val_accuracy: 0.3090 - val_loss: 1.9436\n",
      "Epoch 9/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.3672 - loss: 1.7763 - val_accuracy: 0.3120 - val_loss: 1.9350\n",
      "Epoch 10/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.3798 - loss: 1.7419 - val_accuracy: 0.3162 - val_loss: 1.9275\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 11:13:58.078137: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2215', 524 bytes spill stores, 524 bytes spill loads\n",
      "\n",
      "2025-05-06 11:13:58.103433: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2215', 2704 bytes spill stores, 2704 bytes spill loads\n",
      "\n",
      "2025-05-06 11:13:58.288920: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2215', 6320 bytes spill stores, 6276 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0985 - loss: 3.0412"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 11:13:59.971646: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2215', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2025-05-06 11:14:00.059187: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2215', 124 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2025-05-06 11:14:00.144543: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2215_0', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2025-05-06 11:14:00.171647: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2215', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-05-06 11:14:00.194322: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2215', 208 bytes spill stores, 208 bytes spill loads\n",
      "\n",
      "2025-05-06 11:14:00.207051: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2215', 652 bytes spill stores, 652 bytes spill loads\n",
      "\n",
      "2025-05-06 11:14:00.338650: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2215', 124 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2025-05-06 11:14:00.360224: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2215', 1088 bytes spill stores, 1128 bytes spill loads\n",
      "\n",
      "2025-05-06 11:14:00.505143: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2215', 4932 bytes spill stores, 4948 bytes spill loads\n",
      "\n",
      "2025-05-06 11:14:00.592832: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2215', 6604 bytes spill stores, 6600 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 67ms/step - accuracy: 0.1006 - loss: 3.0033 - val_accuracy: 0.1454 - val_loss: 2.4287\n",
      "Epoch 2/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1568 - loss: 2.3567 - val_accuracy: 0.2036 - val_loss: 2.1822\n",
      "Epoch 3/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2043 - loss: 2.1724 - val_accuracy: 0.2382 - val_loss: 2.0896\n",
      "Epoch 4/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2298 - loss: 2.0886 - val_accuracy: 0.2548 - val_loss: 2.0306\n",
      "Epoch 5/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2480 - loss: 2.0341 - val_accuracy: 0.2740 - val_loss: 1.9875\n",
      "Epoch 6/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2620 - loss: 1.9942 - val_accuracy: 0.2886 - val_loss: 1.9533\n",
      "Epoch 7/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2768 - loss: 1.9625 - val_accuracy: 0.2996 - val_loss: 1.9251\n",
      "Epoch 8/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2913 - loss: 1.9358 - val_accuracy: 0.3100 - val_loss: 1.9007\n",
      "Epoch 9/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3029 - loss: 1.9122 - val_accuracy: 0.3216 - val_loss: 1.8794\n",
      "Epoch 10/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3124 - loss: 1.8909 - val_accuracy: 0.3304 - val_loss: 1.8603\n",
      "Epoch 11/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3200 - loss: 1.8716 - val_accuracy: 0.3354 - val_loss: 1.8433\n",
      "Epoch 12/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3275 - loss: 1.8539 - val_accuracy: 0.3428 - val_loss: 1.8279\n",
      "Epoch 13/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3344 - loss: 1.8378 - val_accuracy: 0.3490 - val_loss: 1.8143\n",
      "Epoch 14/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3399 - loss: 1.8229 - val_accuracy: 0.3546 - val_loss: 1.8021\n",
      "Epoch 15/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3462 - loss: 1.8092 - val_accuracy: 0.3570 - val_loss: 1.7912\n",
      "Epoch 16/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3516 - loss: 1.7967 - val_accuracy: 0.3610 - val_loss: 1.7813\n",
      "Epoch 17/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3564 - loss: 1.7849 - val_accuracy: 0.3630 - val_loss: 1.7721\n",
      "Epoch 18/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3601 - loss: 1.7741 - val_accuracy: 0.3662 - val_loss: 1.7636\n",
      "Epoch 19/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3635 - loss: 1.7638 - val_accuracy: 0.3676 - val_loss: 1.7556\n",
      "Epoch 20/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3674 - loss: 1.7542 - val_accuracy: 0.3672 - val_loss: 1.7481\n",
      "Epoch 21/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3713 - loss: 1.7451 - val_accuracy: 0.3696 - val_loss: 1.7410\n",
      "Epoch 22/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3743 - loss: 1.7365 - val_accuracy: 0.3708 - val_loss: 1.7341\n",
      "Epoch 23/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3776 - loss: 1.7283 - val_accuracy: 0.3720 - val_loss: 1.7276\n",
      "Epoch 24/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3814 - loss: 1.7204 - val_accuracy: 0.3754 - val_loss: 1.7213\n",
      "Epoch 25/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3835 - loss: 1.7128 - val_accuracy: 0.3772 - val_loss: 1.7151\n",
      "Epoch 1/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 69ms/step - accuracy: 0.1021 - loss: 2.9019 - val_accuracy: 0.0956 - val_loss: 3.4672\n",
      "Epoch 2/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1291 - loss: 2.7019 - val_accuracy: 0.1430 - val_loss: 2.8087\n",
      "Epoch 3/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1544 - loss: 2.5573 - val_accuracy: 0.1702 - val_loss: 2.5569\n",
      "Epoch 4/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1761 - loss: 2.4502 - val_accuracy: 0.1876 - val_loss: 2.4301\n",
      "Epoch 5/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1946 - loss: 2.3657 - val_accuracy: 0.2062 - val_loss: 2.3546\n",
      "Epoch 6/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2098 - loss: 2.2965 - val_accuracy: 0.2166 - val_loss: 2.3011\n",
      "Epoch 7/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2246 - loss: 2.2379 - val_accuracy: 0.2294 - val_loss: 2.2604\n",
      "Epoch 8/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2407 - loss: 2.1881 - val_accuracy: 0.2382 - val_loss: 2.2282\n",
      "Epoch 9/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2530 - loss: 2.1451 - val_accuracy: 0.2422 - val_loss: 2.1999\n",
      "Epoch 10/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2628 - loss: 2.1073 - val_accuracy: 0.2524 - val_loss: 2.1746\n",
      "Epoch 11/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2718 - loss: 2.0735 - val_accuracy: 0.2584 - val_loss: 2.1517\n",
      "Epoch 12/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2800 - loss: 2.0432 - val_accuracy: 0.2598 - val_loss: 2.1303\n",
      "Epoch 13/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2888 - loss: 2.0158 - val_accuracy: 0.2654 - val_loss: 2.1107\n",
      "Epoch 14/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2965 - loss: 1.9910 - val_accuracy: 0.2700 - val_loss: 2.0922\n",
      "Epoch 15/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3039 - loss: 1.9685 - val_accuracy: 0.2786 - val_loss: 2.0751\n",
      "Epoch 16/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3098 - loss: 1.9477 - val_accuracy: 0.2812 - val_loss: 2.0598\n",
      "Epoch 17/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3158 - loss: 1.9285 - val_accuracy: 0.2876 - val_loss: 2.0457\n",
      "Epoch 18/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3219 - loss: 1.9106 - val_accuracy: 0.2892 - val_loss: 2.0323\n",
      "Epoch 19/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3278 - loss: 1.8939 - val_accuracy: 0.2932 - val_loss: 2.0200\n",
      "Epoch 20/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3335 - loss: 1.8782 - val_accuracy: 0.2974 - val_loss: 2.0089\n",
      "Epoch 21/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3390 - loss: 1.8634 - val_accuracy: 0.2992 - val_loss: 1.9983\n",
      "Epoch 22/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3443 - loss: 1.8495 - val_accuracy: 0.3024 - val_loss: 1.9884\n",
      "Epoch 23/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3481 - loss: 1.8362 - val_accuracy: 0.3056 - val_loss: 1.9791\n",
      "Epoch 24/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3534 - loss: 1.8237 - val_accuracy: 0.3088 - val_loss: 1.9704\n",
      "Epoch 25/25\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3589 - loss: 1.8116 - val_accuracy: 0.3098 - val_loss: 1.9621\n"
     ]
    }
   ],
   "source": [
    "# batch=32, epochs=10\n",
    "nn0_3210 = build_model(batch_norm = False)\n",
    "nn1_3210 = build_model(batch_norm = True)\n",
    "hist0_3210 = compile_and_fit(model=nn0_3210, batch_size=32, total_epochs=10)\n",
    "hist1_3210 = compile_and_fit(model=nn1_3210, batch_size=32, total_epochs=10)\n",
    "\n",
    "#batch=1024, epochs=25\n",
    "nn0_102425 = build_model(batch_norm = False)\n",
    "nn1_102425 = build_model(batch_norm = True)\n",
    "hist0_102425 = compile_and_fit(model=nn0_102425, batch_size=1024, total_epochs=25)\n",
    "hist1_102425 = compile_and_fit(model=nn1_102425, batch_size=1024, total_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28b1844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval0_3210 = evaluate_model(nn0_3210, batch_size=32)\n",
    "eval1_3210 = evaluate_model(nn1_3210, batch_size=32)\n",
    "\n",
    "eval0_102425 = evaluate_model(nn0_102425, batch_size=1024)\n",
    "eval1_102425 = evaluate_model(nn1_102425, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb0a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy in the last epoch: 0.446911096572876\n",
      "Tranning accuracy using model evaluate: 0.45251110196113586\n",
      "Validation accuracy in the last epoch: 0.43299999833106995\n",
      "Validation accuracy using model evaluate: 0.43299999833106995\n"
     ]
    }
   ],
   "source": [
    "# Without batch norm, batch=32, epochs=10\n",
    "print(\"Training accuracy in the last epoch:\", hist0_3210.history['accuracy'][-1])\n",
    "print(\"Tranning accuracy using model evaluate:\", eval0_3210[0])\n",
    "print(\"Validation accuracy in the last epoch:\", hist0_3210.history['val_accuracy'][-1])\n",
    "print(\"Validation accuracy using model evaluate:\", eval0_3210[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2c544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy in the last epoch: 0.38588887453079224\n",
      "Tranning accuracy using model evaluate: 0.32988888025283813\n",
      "Validation accuracy in the last epoch: 0.31619998812675476\n",
      "Validation accuracy using model evaluate: 0.31619998812675476\n"
     ]
    }
   ],
   "source": [
    "# With batch norm, batch=32, epochs=10\n",
    "print(\"Training accuracy in the last epoch:\", hist1_3210.history['accuracy'][-1])\n",
    "print(\"Tranning accuracy using model evaluate:\", eval1_3210[0])\n",
    "print(\"Validation accuracy in the last epoch:\", hist1_3210.history['val_accuracy'][-1])\n",
    "print(\"Validation accuracy using model evaluate:\", eval1_3210[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f335d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy in the last epoch: 0.38457778096199036\n",
      "Tranning accuracy using model evaluate: 0.38642221689224243\n",
      "Validation accuracy in the last epoch: 0.37720000743865967\n",
      "Validation accuracy using model evaluate: 0.37720000743865967\n"
     ]
    }
   ],
   "source": [
    "# Without batch norm, batch=1024, epochs=25\n",
    "print(\"Training accuracy in the last epoch:\", hist0_102425.history['accuracy'][-1])\n",
    "print(\"Tranning accuracy using model evaluate:\", eval0_102425[0])\n",
    "print(\"Validation accuracy in the last epoch:\", hist0_102425.history['val_accuracy'][-1])\n",
    "print(\"Validation accuracy using model evaluate:\", eval0_102425[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a77a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy in the last epoch: 0.3581777811050415\n",
      "Tranning accuracy using model evaluate: 0.3585111200809479\n",
      "Validation accuracy in the last epoch: 0.30979999899864197\n",
      "Validation accuracy using model evaluate: 0.30979999899864197\n"
     ]
    }
   ],
   "source": [
    "# With batch norm, batch=1024, epochs=25\n",
    "print(\"Training accuracy in the last epoch:\", hist1_102425.history['accuracy'][-1])\n",
    "print(\"Tranning accuracy using model evaluate:\", eval1_102425[0])\n",
    "print(\"Validation accuracy in the last epoch:\", hist1_102425.history['val_accuracy'][-1])\n",
    "print(\"Validation accuracy using model evaluate:\", eval1_102425[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
